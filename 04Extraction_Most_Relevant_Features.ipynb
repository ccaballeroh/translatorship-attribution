{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ccaballeroh/MCPR-2021/blob/main/04Extraction_Most_Relevant_Features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-jZ3uI9mRKh2"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1HK4MsUARPCG"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    ROOT = Path(r\"./drive/My Drive/Translator-Attribution\")\n",
    "    sys.path.insert(0,f\"{ROOT}/\")\n",
    "else:\n",
    "    from helper.analysis import ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxRkXRnu6ymW"
   },
   "source": [
    "# Extraction of Most Relevant Features\n",
    "\n",
    "On this notebook, we extract the most relevant features in the classification process for each translator. In order to do this, we can retrieve the learned weights from a linear classifier (e.g., Logistic Regression, although a Support Vector Machine using a linear *kernel* also have those properties as well as the Na√Øve Bayes classifier) and get the $n$ largest. The corresponding $n$ features would thus be the most relevant for each class. In case of a binary classifier, the $n$ largest weights would correspond to the *positive* class, whereas the $n$ most negative weights would correspond to the *negative* class.\n",
    "\n",
    "Since scikit-learn trains $N$ binary classifiers when given an N-class multiclass problem, we can retrieve the $n$ largest weights&mdash;and their corresponding features&mdash;for each classifier. This notebook saves to disk the $n$ most relevant features for each translator in the corpora for each feature set for a logistic regression classifier. The results are saved as bar plots and also tabular ($\\LaTeX$) in the `results\\figs\\most` and `results\\tables` folders respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "COM-qODNRCqx"
   },
   "outputs": [],
   "source": [
    "from helper.features import convert_data, plot_most_relevant, train_extract_most_relevant, save_tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkkRVjg86ymq"
   },
   "source": [
    "These are the files to process. They are the entirety of the feature sets obtained using [01Processing](./01Processing.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4F_KDYeF6ymr"
   },
   "outputs": [],
   "source": [
    "from helper.analysis import JSON_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2spuxmx76ymv"
   },
   "source": [
    "## Most Relevant Features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpRIncSsLSfJ"
   },
   "source": [
    "We use $\\chi^2$ statistic to select the $k$ most distinctive features in each feature set to train a Logistic Regression classifier. Then we extract the $n$ most important feature for each class (i.e., translator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zOJ1kKwV6ym1",
    "outputId": "17cfc26a-6480-4424-ca63-364ec4fab218"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "most_relevant = {}\n",
    "num_of_features = 25\n",
    "n_most_relevant = 10\n",
    "args = {\"k\":num_of_features, \"n\":n_most_relevant}\n",
    "\n",
    "for author in [\"Ibsen\", \"Quixote\"]:\n",
    "    features_files = [file for file in JSON_FOLDER.iterdir() if file.name.startswith(author)]\n",
    "    for file in features_files:\n",
    "        data = convert_data(file=file)\n",
    "        data = {**data, **args}\n",
    "        most_relevant[file.stem] = train_extract_most_relevant(**data) \n",
    "\n",
    "        for translator in data[\"encoder\"].classes_:\n",
    "            plot_most_relevant(data=most_relevant[file.stem], translator=translator, file=file)\n",
    "            df = most_relevant[file.stem][translator]\n",
    "            save_tables(df=df, translator=translator, file=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWTGeRlrLSfK"
   },
   "source": [
    "## Results\n",
    "\n",
    "As an example, we can compare bigrams among the parallel translations of Ibsen (i.e., *Ghosts*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "s-iAEVvtLSfK"
   },
   "outputs": [],
   "source": [
    "key = \"_\".join(\n",
    "    [\"Ibsen\",\n",
    "     \"Ghosts\",\n",
    "     \"2grams\",\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "eatDiYQTLSfL",
    "outputId": "a3448d11-af3d-4477-f084-c1bf23ef1d00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharp :\n",
      "         Feature    Weight\n",
      "1       was the  0.707787\n",
      "2        do n't  0.623912\n",
      "3         as if  0.474847\n",
      "4         it is  0.452557\n",
      "5   manders but  0.445357\n",
      "6         up to  0.444372\n",
      "7   manders and  0.387682\n",
      "8          i am  0.207958\n",
      "9        at all  0.091415\n",
      "10     going to  0.034201 \n",
      "\n",
      "\n",
      "Archer :\n",
      "        Feature    Weight\n",
      "1         i 'm  0.790354\n",
      "2       do not  0.729262\n",
      "3        i 've  0.686770\n",
      "4    PROPN why  0.583256\n",
      "5   PROPN then  0.490356\n",
      "6      can not  0.455052\n",
      "7    not PROPN  0.451855\n",
      "8     no doubt  0.407669\n",
      "9     there 's  0.354415\n",
      "10     you 're  0.272801 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for translator in most_relevant[key].keys():\n",
    "    print(translator, \":\\n\", most_relevant[key][translator], 2*\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wk58oatMLSfL"
   },
   "source": [
    "Another example, from the other corpus. We show the 10 most distinctive cohesive markers sorrounded by their corresponding punctuation marks for each translator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hYCWbQ4GLSfM",
    "outputId": "85e57210-f554-4d44-b6e4-e70cb8ae6304"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jarvis :\n",
      "         Feature    Weight\n",
      "1        ; and,  1.783961\n",
      "2         ; and  1.746763\n",
      "3   . in short,  1.406864\n",
      "4         . and  0.803417\n",
      "5          also  0.593942\n",
      "6       , since  0.516417\n",
      "7         : and  0.460105\n",
      "8   immediately  0.409644\n",
      "9         \" and  0.195828\n",
      "10    therefore  0.171831 \n",
      "\n",
      "\n",
      "Ormsby :\n",
      "        Feature    Weight\n",
      "1          and  2.397842\n",
      "2   , however,  1.361017\n",
      "3         \"but  0.383121\n",
      "4         \"and  0.351197\n",
      "5        \" and  0.325242\n",
      "6      , while  0.286503\n",
      "7     although  0.138443\n",
      "8       'there -0.001208\n",
      "9   , although -0.016483\n",
      "10        'but -0.265750 \n",
      "\n",
      "\n",
      "Shelton :\n",
      "        Feature    Weight\n",
      "1         'and  1.711369\n",
      "2        , yet  1.617163\n",
      "3         'but  0.909684\n",
      "4     likewise  0.904244\n",
      "5   , although  0.799222\n",
      "6       'there  0.633918\n",
      "7       , and,  0.511501\n",
      "8      , since  0.480780\n",
      "9     although  0.477891\n",
      "10   therefore  0.429038 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "key = \"_\".join(\n",
    "    [\"Quixote\",\n",
    "     \"cohesive\",\n",
    "     \"punct\"       \n",
    "    ])\n",
    "for translator in most_relevant[key].keys():\n",
    "    print(translator, \":\\n\", most_relevant[key][translator], 2*\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "04Extraction_Most_Relevant_Features.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:cohesive-markers]",
   "language": "python",
   "name": "conda-env-cohesive-markers-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
